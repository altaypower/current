{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35843a8",
   "metadata": {},
   "source": [
    "Конечный ноутбук"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a220fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f51b3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score,\\\n",
    "         recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7f1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm\n",
    "import  catboost as catb\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ab164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753446c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.0.1.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715c6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcffc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70c92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
    "    print('TRAIN\\n\\n' + classification_report(y_train_true, y_train_pred))\n",
    "    print('TEST\\n\\n' + classification_report(y_test_true, y_test_pred))\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7996f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    get_classification_report(y_train, y_train_pred, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f47f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df_by_target(df, target_name):\n",
    "\n",
    "    target_counts = df[target_name].value_counts()\n",
    "\n",
    "    major_class_name = target_counts.argmax()\n",
    "    minor_class_name = target_counts.argmin()\n",
    "\n",
    "    disbalance_coeff = int(target_counts[major_class_name] / target_counts[minor_class_name]) - 1\n",
    "\n",
    "    for i in range(disbalance_coeff):\n",
    "        sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name])\n",
    "        df = df.append(sample, ignore_index=True)\n",
    "\n",
    "    return df.sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81022095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 # подсчитываем память потребляемую изначальным датасетом\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns: # проходимся по всем колонкам\n",
    "        col_type = df[col].dtype  # узнаем тип колонки\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min() # смотрим минимальное значение признака\n",
    "            c_max = df[col].max() # смотрим максимальное значение признака\n",
    "            if str(col_type)[:3] == 'int':  # if int\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max: # сравниваем с int8\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max: # сравниваем с int16 и.т.д.\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else: # если был тип object, то меняем его тип на пандасовский тип 'category', на нем разные агрегации данных работают в разы быстрее\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2  # считаем сколько теперь у нас занято памяти\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))  # и выводим статистику\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfefe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dd.read_csv('features.csv', blocksize=25e6, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec92385",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features.loc[features['id'].isin(data_train['id'])].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f342ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "        \n",
    "    def fit(self, data_train, features_train):\n",
    "\n",
    "        return data_train, features_train\n",
    "        \n",
    "    def transform(self, data_train, features_train):\n",
    "        \"\"\"Трансформация данных\"\"\"\n",
    "        \n",
    "        dublicated = features_train[features_train['id'].duplicated(keep=False)].sort_values(by='id')['id'].value_counts()\n",
    "        data_merged_train = pd.merge(data_train, features_train, on='id')\n",
    "        tmp = data_merged_train.loc[data_merged_train['id'].isin(dublicated.index)]\n",
    "        tmp['delta'] = abs(tmp['buy_time_x'] - tmp['buy_time_y'])\n",
    "        tmp.sort_values(by = ['Unnamed: 0_x', 'delta'], inplace=True)\n",
    "        duplicates = tmp['Unnamed: 0_x'].duplicated()\n",
    "        duplicates_to_delete = duplicates[duplicates.values == True]\n",
    "        data_merged_train.drop(duplicates_to_delete.index, axis=0, inplace=True)\n",
    "        data_merged_train['time_delta'] = data_merged_train['buy_time_x'] - data_merged_train['buy_time_y']\n",
    "        data_merged_train.drop(['Unnamed: 0_x', 'Unnamed: 0_y', 'buy_time_y'], axis=1, inplace=True)\n",
    "\n",
    "        return data_merged_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapreprocessing = DataPreprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66056751",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_train = datapreprocessing.transform(data_train, features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_train = reduce_mem_usage(data_merged_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794c2c1",
   "metadata": {},
   "source": [
    "Закладка а-ля  Luigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c49a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_train.to_csv('train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f87bf4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bf8c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featuregenerator():    \n",
    "    \n",
    "    def __init__(self):          \n",
    "        self.log_df=None\n",
    "        self.action_model = None\n",
    "        \n",
    "    def fit(self, df_all, start, stop):\n",
    "        from datetime import datetime, date, time\n",
    "        from datetime import timedelta\n",
    "        import holidays\n",
    "        from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "    \n",
    "        X = df_all.drop(columns=['target'])\n",
    "        y = df_all['target']\n",
    "        # признаки логирования количества предложений подключения услуги юзеру\n",
    "        df = X[['id', 'vas_id']]    \n",
    "        df['vas_id_01'] = 0 \n",
    "        df['vas_id_02'] = 0\n",
    "        df['vas_id_04'] = 0\n",
    "        df['vas_id_05'] = 0\n",
    "        df['vas_id_06'] = 0\n",
    "        df['vas_id_07'] = 0\n",
    "        df['vas_id_08'] = 0\n",
    "        df['vas_id_09'] = 0\n",
    "        \n",
    "        df.loc[df['vas_id']==1.0,'vas_id_01'] = 1\n",
    "        df.loc[df['vas_id']==2.0,'vas_id_02'] = 1\n",
    "        df.loc[df['vas_id']==4.0,'vas_id_04'] = 1\n",
    "        df.loc[df['vas_id']==5.0,'vas_id_05'] = 1\n",
    "        df.loc[df['vas_id']==6.0,'vas_id_06'] = 1\n",
    "        df.loc[df['vas_id']==7.0,'vas_id_07'] = 1\n",
    "        df.loc[df['vas_id']==8.0,'vas_id_08'] = 1\n",
    "        df.loc[df['vas_id']==9.0,'vas_id_09'] = 1\n",
    "                \n",
    "        self.log_df = df.groupby(['id', 'vas_id'], as_index=False)['vas_id_01', 'vas_id_02', 'vas_id_04', 'vas_id_05',\n",
    "                                                                  'vas_id_06', 'vas_id_07', 'vas_id_08', 'vas_id_09'].sum() \n",
    "        X = X.reset_index().merge(self.log_df, on=['id', 'vas_id'], how='left').set_index('index').fillna(0)\n",
    "        list_id = list(X.id.unique())\n",
    "        for i in list_id:\n",
    "            ix = X.loc[X['id']==i,'vas_id'].value_counts()\n",
    "            for k in ix.index:\n",
    "                X.loc[X['id']==i,'vas_id_0'+str(int(k))] += ix[k]\n",
    "                \n",
    "        # Добываю максимум информации из 'buy_time_x', остальные признаки, включая выходные и праздники\n",
    "        # к увеличению метрик не привели\n",
    "        X['date'] = list(map(datetime.fromtimestamp,X['buy_time_x']))\n",
    "        X['month'] = X['date'].apply(lambda x: x.timetuple()[1])\n",
    "        X['day'] = X['date'].apply(lambda x: x.timetuple()[7])\n",
    "        X['weekofyear'] = X['buy_time_x'].apply(lambda x: pd.to_datetime(date.fromtimestamp(x)).weekofyear)\n",
    "        \n",
    "        X['time_max'] = X.buy_time_x.max()\n",
    "        X['novelty'] = X['time_max'] - X['buy_time_x']\n",
    "        \n",
    "        few = pd.DataFrame(X['id'].value_counts()) \n",
    "        few = few.loc[few['id']>1] # через функцию df.apply() дождаться результата нереально, пришлось колхозить\n",
    "\n",
    "        # vas_id_day это период между первым и последним предложениями, признак необходим для бизнес-логики\n",
    "        X['vas_id_day'] = 0 \n",
    "        for i in few.index:\n",
    "            ix = X.loc[(X['id']==i)].sort_values(by='buy_time_x', ascending=True)\n",
    "            for k in range(1, ix.shape[0]):\n",
    "                df1 = X.loc[ix.index[k-1],'date'] # предыдущее предложение\n",
    "                df2 = X.loc[ix.index[k],'date'] # первое предложение\n",
    "                X.loc[ix.index[k],'vas_id_day'] = (pd.to_datetime(df2)-pd.to_datetime(df1)).days #  + 1 этого не было, только здесь добавила\n",
    "                if (k==2) & ((pd.to_datetime(df2)-pd.to_datetime(df1)).days==0):\n",
    "                    X.loc[ix.index[k],'vas_id_day'] = X.loc[ix.index[k-1],'vas_id_day']\n",
    "        # Обучаю модель Catboost вычислять юзеров, покупающих услуги по акции (на выбросе)            \n",
    "        \"\"\"Обучаем Catboost\"\"\"\n",
    "        X['is_action'] = 0\n",
    "        X.loc[((X['day'] > start) & (X['day'] < stop) & (y==1)), 'is_action'] = 1\n",
    "        X_action = X.drop(columns=['is_action'])\n",
    "        y_action = X['is_action']\n",
    "        disbalance = y_action.value_counts()[0] / y_action.value_counts()[1]\n",
    "        frozen_params = {\n",
    "             'class_weights':[1, disbalance], \n",
    "             'silent':True,\n",
    "             'random_state':21,\n",
    "             'eval_metric':'F1',\n",
    "             'early_stopping_rounds':60\n",
    "        }\n",
    "        self.action_model = catb.CatBoostClassifier(**frozen_params)                                             \n",
    "        self.action_model.fit(X_action, y_action)\n",
    "        y_pred = self.action_model.predict(X)\n",
    "        X['is_action'] = y_pred\n",
    "        X.drop(columns=['date'], inplace=True)\n",
    "        train_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "        \n",
    "        return train_df\n",
    "        \n",
    "    def transform(self, df_all):\n",
    "        from datetime import datetime, date, time\n",
    "        from datetime import timedelta\n",
    "        import holidays\n",
    "        from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "        X = df_all.drop(columns=['target'])\n",
    "        y = df_all['target']\n",
    "        # добавляю количество предложений юзеру из тестового датасета\n",
    "        # к уже записанным предложениям из тренировочного датасета\n",
    "        X = X.reset_index().merge(self.log_df, on=['id', 'vas_id'], how='left').set_index('index').fillna(0)\n",
    "        list_id = list(X.id.unique())\n",
    "        for i in list_id:\n",
    "            ix = X.loc[X['id']==i,'vas_id'].value_counts()\n",
    "            for k in ix.index:\n",
    "                X.loc[X['id']==i,'vas_id_0'+str(int(k))] += ix[k]\n",
    "                \n",
    "        # Добываю максимум информации из 'buy_time_x', остальные признаки, включая выходные и праздники\n",
    "        # к увеличению метрик не привели\n",
    "        X['date'] = list(map(datetime.fromtimestamp,X['buy_time_x']))\n",
    "        X['month'] = X['date'].apply(lambda x: x.timetuple()[1])\n",
    "        X['day'] = X['date'].apply(lambda x: x.timetuple()[7])\n",
    "        X['weekofyear'] = X['buy_time_x'].apply(lambda x: pd.to_datetime(date.fromtimestamp(x)).weekofyear)\n",
    "        \n",
    "        X['time_max'] = X.buy_time_x.max()\n",
    "        X['novelty'] = X['time_max'] - X['buy_time_x']\n",
    "        \n",
    "        few = pd.DataFrame(X['id'].value_counts()) \n",
    "        few = few.loc[few['id']>1] # через функцию df.apply() дождаться результата нереально, пришлось колхозить\n",
    "\n",
    "        # vas_id_day это период между первым и последним предложениями, признак необходим для бизнес-логики\n",
    "        X['vas_id_day'] = 0 \n",
    "        for i in few.index:\n",
    "            ix = X.loc[(X['id']==i)].sort_values(by='buy_time_x', ascending=True)\n",
    "            for k in range(1, ix.shape[0]):\n",
    "                df1 = X.loc[ix.index[k-1],'date'] # предыдущее предложение\n",
    "                df2 = X.loc[ix.index[k],'date'] # первое предложение\n",
    "                X.loc[ix.index[k],'vas_id_day'] = (pd.to_datetime(df2)-pd.to_datetime(df1)).days #  + 1 этого не было, только здесь добавила\n",
    "                if (k==2) & ((pd.to_datetime(df2)-pd.to_datetime(df1)).days==0):\n",
    "                    X.loc[ix.index[k],'vas_id_day'] = X.loc[ix.index[k-1],'vas_id_day']\n",
    "        \n",
    "        y_pred = self.action_model.predict(X)\n",
    "        X['is_action'] = y_pred\n",
    "        X.drop(columns=['date'], inplace=True)\n",
    "        X = X.fillna(0)\n",
    "        test_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "        \n",
    "        return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f97b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.92763\n",
       "1.0    0.07237\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0    0.927632\n",
       "1.0    0.072368\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = train_test_split(df_all, \n",
    "                              shuffle=True,\n",
    "                              test_size=0.4,\n",
    "                              random_state=21,\n",
    "                              stratify=df_all['target'])\n",
    "display(train['target'].value_counts(normalize=True), test['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fea3418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuregenerator = Featuregenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b50b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15680/2436705923.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_01'] = 0\n",
      "/tmp/ipykernel_15680/2436705923.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_02'] = 0\n",
      "/tmp/ipykernel_15680/2436705923.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_04'] = 0\n",
      "/tmp/ipykernel_15680/2436705923.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_05'] = 0\n",
      "/tmp/ipykernel_15680/2436705923.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_06'] = 0\n",
      "/tmp/ipykernel_15680/2436705923.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_07'] = 0\n",
      "/tmp/ipykernel_15680/2436705923.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_08'] = 0\n",
      "/tmp/ipykernel_15680/2436705923.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vas_id_09'] = 0\n",
      "/home/jb/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "/tmp/ipykernel_15680/2436705923.py:35: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  self.log_df = df.groupby(['id', 'vas_id'], as_index=False)['vas_id_01', 'vas_id_02', 'vas_id_04', 'vas_id_05',\n"
     ]
    }
   ],
   "source": [
    "train_df = featuregenerator.fit(train, 316, 337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6dccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = featuregenerator.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4a3fa",
   "metadata": {},
   "source": [
    "Закладка а-ля  Luigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12bd1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закладка\n",
    "train_df.to_csv('train_df.csv', index=False)\n",
    "test_df.to_csv('test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dd922d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb04fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb4f32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    def __init__(self):          \n",
    "        self.TARGET_NAME = 'target'\n",
    "        # отобранные фичи из общего датасета для первого классификатора (vas_id==1)\n",
    "        self.columns1 = ['id', 'vas_id', 'buy_time_x', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                 '10', '11', '12', '13', '14', '18', '19', '20', '21', '25', '26', '28', '30',\n",
    "                 '34', '36', '37', '38', '39', '40', '41', '43', '44', '45', '46', '47', '48',\n",
    "                 '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61',\n",
    "                 '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '73', '74', '76',\n",
    "                 '77', '92', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105',\n",
    "                 '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116',\n",
    "                 '117', '121', '123', '124', '125', '126', '127', '128', '129', '130', '131',\n",
    "                 '132', '133', '134', '135', '136', '137', '138', '140', '141', '142', '143',\n",
    "                 '144', '145', '146', '147', '148', '149', '150', '151', '152', '156', '157',\n",
    "                 '158', '159', '160', '161', '162', '164', '165', '166', '167', '168', '169',\n",
    "                 '170', '171', '172', '174', '175', '176', '181', '182', '183', '184', '185',\n",
    "                 '186', '187', '188', '189', '190', '191', '192', '193', '195', '196', '198',\n",
    "                 '200', '201', '202', '204', '205', '207', '208', '209', '210', '211', '212',\n",
    "                 '213', '214', '215', '220', '222', '223', '224', '225', '226', '227', '228',\n",
    "                 '229', '230', '231', '233', '234', '235', '236', '237', '238', '239', '240',\n",
    "                 '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251',\n",
    "                 '252', 'time_delta', 'vas_id_01', 'weekofyear', 'vas_id_day', 'is_action']\n",
    "        # инициализирую классификатор для первого датасета (vas_id==1)\n",
    "        self.classifier1 = None\n",
    "        self.columns2 = ['id', 'vas_id', 'buy_time_x', '0', '1', '2', '3', '4', '5', '7', '8',\n",
    "                  '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21',\n",
    "                  '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
    "                  '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47',\n",
    "                  '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                  '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73',\n",
    "                  '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86',\n",
    "                  '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99',\n",
    "                  '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110',\n",
    "                  '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121',\n",
    "                  '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132',\n",
    "                  '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143',\n",
    "                  '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154',\n",
    "                  '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165',\n",
    "                  '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176',\n",
    "                  '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187',\n",
    "                  '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                  '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209',\n",
    "                  '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220',\n",
    "                  '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231',\n",
    "                  '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242',\n",
    "                  '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', 'time_delta',\n",
    "                  'vas_id_01', 'vas_id_02', 'vas_id_04', 'vas_id_05', 'vas_id_06', 'vas_id_07',\n",
    "                  'vas_id_08', 'vas_id_09', 'month', 'day', 'weekofyear', 'time_max', 'novelty',\n",
    "                  'vas_id_day', 'is_action']\n",
    "        self.classifier2 = None\n",
    "        self.columns4 = ['id', 'vas_id', 'buy_time_x', '0', '2', '4', '6', '8', '10', '11', '12', '14', '18',\n",
    "                      '19', '20', '28', '30', '36', '38', '39', '40', '43', '44', '45', '50',\n",
    "                      '51', '54', '59', '60', '61', '62', '63', '64', '66', '67', '69', '70',\n",
    "                      '72', '73', '76', '82', '86', '110', '111', '112', '113', '114', '115', '116',\n",
    "                      '126', '127', '128', '131', '132', '133', '135', '136', '137', '138',\n",
    "                      '140', '143', '147', '148', '149', '150', '151', '152', '153', '156', '158',\n",
    "                      '159', '160', '162', '164', '173', '176', '186', '188', '190', '192', '193',\n",
    "                      '194', '195', '196', '198', '200', '205', '220', '233', '249', '251', '252',\n",
    "                      'vas_id_04', 'weekofyear', 'novelty', 'vas_id_day', 'is_action']\n",
    "        self.classifier4 = None\n",
    "        self.columns5 = ['id', 'vas_id', 'buy_time_x', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                  '10', '11', '12', '13', '14', '18', '19', '20', '21', '22', '25', '26', '27',\n",
    "                  '28', '29', '30', '34', '36', '37', '38', '39', '40', '41', '42', '43', '44',\n",
    "                  '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55','56',\n",
    "                  '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70',\n",
    "                  '71', '72', '73', '74', '76', '77', '78', '79', '80', '82', '83', '86', '87',\n",
    "                  '89', '90', '91', '92', '94', '96', '97', '98', '99', '100', '101', '102', '103',\n",
    "                  '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                  '115', '116', '117', '118', '119', '120', '121', '123', '124', '125', '126',\n",
    "                  '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137',\n",
    "                  '138', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                  '151', '152', '153', '155', '156', '157', '158', '159', '160', '161', '162', '164',\n",
    "                  '165', '166', '167', '168', '169', '170', '171', '172', '174', '175', '176',\n",
    "                  '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188',\n",
    "                  '189', '190', '191', '192', '193', '194', '195', '196', '198', '199', '200',\n",
    "                  '201', '202', '204', '205', '206', '207', '208', '209', '210', '211', '212',\n",
    "                  '213', '214', '215', '217', '219', '220', '222', '223', '224', '225', '226',\n",
    "                  '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237',\n",
    "                  '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248',\n",
    "                  '249', '250', '251', '252', 'time_delta', 'vas_id_05', 'month', 'day',\n",
    "                  'weekofyear', 'novelty', 'vas_id_day', 'is_action']\n",
    "        self.classifier5 = None\n",
    "        self.columns6 = ['id', 'vas_id', 'buy_time_x', '0', '2', '4', '6', '8', '10', '11', '12', '14', '17', '18',\n",
    "                        '19', '20', '22', '26', '27', '28', '29', '30', '31', '35', '38', '39', '40',\n",
    "                        '43', '44', '45', '50', '51', '54', '60', '61', '62', '63', '64', '65', '66',\n",
    "                        '69', '70', '71', '72', '73', '83', '86', '87', '88', '96', '101', '105', '110',\n",
    "                        '111', '112', '113', '114', '115', '116', '123', '124', '125', '126', '127',\n",
    "                        '128', '129', '132', '133', '135', '136', '137', '138', '141', '142', '143',\n",
    "                        '148', '149', '150', '151', '152', '153', '156', '157', '159', '160', '161',\n",
    "                        '162', '163', '175', '176', '177', '178', '186', '188', '189', '190', '192',\n",
    "                        '194', '195', '196', '198', '200', '201', '202', '204', '205', '206', '209',\n",
    "                        '214', '217', '219', '220', '252', 'vas_id_06', 'month', 'day', 'novelty',\n",
    "                        'vas_id_day', 'is_action']\n",
    "        self.classifier6 = None\n",
    "        self.columns7 = ['id', 'vas_id', 'buy_time_x', '0', '2', '4', '6', '8', '10', '11', '12',\n",
    "                     '14', '15', '16', '17', '18', '19', '20', '23', '24', '26', '27', '28', '29',\n",
    "                     '31', '32', '33', '35', '36', '38', '39', '40', '43', '44', '45', '46', '50',\n",
    "                     '51', '54', '57', '60', '61', '62', '63', '64', '65', '66', '67', '69', '70',\n",
    "                     '71', '72', '73', '78', '79', '80', '83', '84', '85', '86', '87', '88', '91',\n",
    "                     '92', '93', '110', '112', '113', '116', '118', '119', '121', '122', '123',\n",
    "                     '124', '125', '126', '128', '131', '133', '135', '136', '137', '143', '147',\n",
    "                     '148', '149', '150', '153', '154', '155', '159', '161', '162', '164', '167',\n",
    "                     '170', '173', '174', '176', '177', '178', '179', '180', '182', '184', '185',\n",
    "                     '186', '188', '192', '193', '194', '195', '196', '197', '198', '199', '200',\n",
    "                     '201', '202', '204', '205', '206', '209', '212', '214', '217', '218', '219',\n",
    "                     '220', '221', '252', 'vas_id_07', 'day', 'novelty', 'vas_id_day', 'is_action',]\n",
    "        self.classifier7 = None\n",
    "        self.columns8 = ['id', 'vas_id', 'buy_time_x', '0', '1', '2', '3', '4', '5', '6', '7',\n",
    "                        '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "                        '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33',\n",
    "                        '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                        '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59',\n",
    "                        '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72',\n",
    "                        '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85',\n",
    "                        '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98',\n",
    "                        '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
    "                        '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120',\n",
    "                        '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131',\n",
    "                        '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142',\n",
    "                        '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153',\n",
    "                        '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164',\n",
    "                        '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175',\n",
    "                        '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                        '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197',\n",
    "                        '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208',\n",
    "                        '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219',\n",
    "                        '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230',\n",
    "                        '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241',\n",
    "                        '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252',\n",
    "                        'time_delta', 'vas_id_01', 'vas_id_02', 'vas_id_04', 'vas_id_05', 'vas_id_06',\n",
    "                        'vas_id_07', 'vas_id_08', 'vas_id_09', 'month', 'day', 'weekofyear', 'time_max',\n",
    "                        'novelty', 'vas_id_day', 'is_action']\n",
    "        self.classifier8 = None\n",
    "        self.columns9 = ['id', 'vas_id', 'buy_time_x', '0', '2', '4', '6', '8', '10', '11', '12', '14', '18', '19',\n",
    "                        '20', '26', '27', '28', '29', '30', '36', '38', '39', '40', '43', '44', '45',\n",
    "                        '50', '51', '54', '61', '62', '63', '64', '65', '66', '67', '71', '72', '73',\n",
    "                        '78', '79', '80', '82', '90', '91', '92', '93', '101', '102', '110', '112', '113',\n",
    "                        '114', '115', '116', '118', '123', '124', '125', '126', '127', '128', '131', '132',\n",
    "                        '133', '135', '136', '137', '138', '140', '142', '143', '148', '149', '150',\n",
    "                        '151', '152', '156', '157', '159', '162', '163', '164', '173', '174', '175',\n",
    "                        '178', '186', '188', '193', '195', '196', '198', '199', '200', '201', '204',\n",
    "                        '209', '214', '217', '219', '231', '252', 'month', 'day', 'novelty', 'vas_id_day',\n",
    "                        'is_action']\n",
    "        self.classifier9 = None\n",
    "        \n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        import pickle\n",
    "        X_train = train_df.drop(columns=[self.TARGET_NAME])\n",
    "        y_train = train_df[self.TARGET_NAME]\n",
    "\n",
    "        train_df_1 = train_df[train_df['vas_id']==1]\n",
    "        X_train1 = train_df_1.drop(columns=['target'])\n",
    "        y_train1 = train_df_1['target']\n",
    "        df_for_balancing = pd.concat([X_train1, y_train1], axis=1)\n",
    "        df_balanced = self.balance_df_by_target(df_for_balancing, self.TARGET_NAME)    \n",
    "        df_balanced[self.TARGET_NAME].value_counts()\n",
    "        X_train1 = df_balanced.drop(columns=self.TARGET_NAME)\n",
    "        y_train1 = df_balanced[self.TARGET_NAME]\n",
    "        # загружаю сохраненный первый классификатор\n",
    "        with open('model1.pkl', 'rb') as model:\n",
    "            self.classifier1 = pickle.load(model)\n",
    "        self.classifier1.fit(X_train1[self.columns1], y_train1)\n",
    "        train_df_2 = train_df[train_df['vas_id']==2]\n",
    "        X_train2 = train_df_2.drop(columns=['target'])\n",
    "        y_train2 = train_df_2['target']\n",
    "        df_for_balancing = pd.concat([X_train2, y_train2], axis=1)\n",
    "        df_balanced = self.balance_df_by_target(df_for_balancing, self.TARGET_NAME)    \n",
    "        df_balanced[self.TARGET_NAME].value_counts()\n",
    "        X_train2 = df_balanced.drop(columns=self.TARGET_NAME)\n",
    "        y_train2 = df_balanced[self.TARGET_NAME]\n",
    "        with open('model2.pkl', 'rb') as model:\n",
    "            self.classifier2 = pickle.load(model)\n",
    "        self.classifier2.fit(X_train2[self.columns2], y_train2)\n",
    "        \n",
    "        train_df_4 = train_df[train_df['vas_id']==4]\n",
    "        X_train4 = train_df_4.drop(columns=['target'])\n",
    "        y_train4 = train_df_4['target']\n",
    "        df_for_balancing = pd.concat([X_train4, y_train4], axis=1)\n",
    "        df_balanced = self.balance_df_by_target(df_for_balancing, self.TARGET_NAME)    \n",
    "        df_balanced[self.TARGET_NAME].value_counts()\n",
    "        X_train4 = df_balanced.drop(columns=self.TARGET_NAME)\n",
    "        y_train4 = df_balanced[self.TARGET_NAME]\n",
    "        with open('model4.pkl', 'rb') as model:\n",
    "            self.classifier4 = pickle.load(model)\n",
    "        self.classifier4.fit(X_train4[self.columns4], y_train4)\n",
    "        \n",
    "        train_df_5 = train_df[train_df['vas_id']==5]\n",
    "        X_train5 = train_df_5.drop(columns=['target'])\n",
    "        y_train5 = train_df_5['target']\n",
    "        with open('model5.pkl', 'rb') as model:\n",
    "            self.classifier5 = pickle.load(model)\n",
    "        self.classifier5.fit(X_train5[self.columns5], y_train5)\n",
    "        \n",
    "        train_df_6 = train_df[train_df['vas_id']==6]\n",
    "        X_train6 = train_df_6.drop(columns=['target'])\n",
    "        y_train6 = train_df_6['target']\n",
    "        with open('model6.pkl', 'rb') as model:\n",
    "            self.classifier6 = pickle.load(model)\n",
    "        self.classifier6.fit(X_train6[self.columns6], y_train6)\n",
    "        \n",
    "        train_df_7 = train_df[train_df['vas_id']==7]\n",
    "        X_train7 = train_df_7.drop(columns=['target'])\n",
    "        y_train7 = train_df_7['target']\n",
    "        df_for_balancing = pd.concat([X_train7, y_train7], axis=1)\n",
    "        df_balanced = self.balance_df_by_target(df_for_balancing, self.TARGET_NAME)    \n",
    "        df_balanced[self.TARGET_NAME].value_counts()\n",
    "        X_train7 = df_balanced.drop(columns=self.TARGET_NAME)\n",
    "        y_train7 = df_balanced[self.TARGET_NAME]\n",
    "        with open('model7.pkl', 'rb') as model:\n",
    "            self.classifier7 = pickle.load(model)\n",
    "        self.classifier7.fit(X_train7[self.columns7], y_train7)\n",
    "        \n",
    "        train_df_8 = train_df[train_df['vas_id']==8]\n",
    "        X_train8 = train_df_8.drop(columns=['target'])\n",
    "        y_train8 = train_df_8['target']\n",
    "        with open('model8.pkl', 'rb') as model:\n",
    "            self.classifier8 = pickle.load(model)\n",
    "        self.classifier8.fit(X_train8[self.columns8], y_train8)\n",
    "        \n",
    "        train_df_9 = train_df[train_df['vas_id']==9]\n",
    "        X_train9 = train_df_9.drop(columns=['target'])\n",
    "        y_train9 = train_df_9['target']\n",
    "        with open('model9.pkl', 'rb') as model:\n",
    "            self.classifier9 = pickle.load(model)\n",
    "        self.classifier9.fit(X_train9[self.columns9], y_train9)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, test_df):        \n",
    "        \n",
    "        X_test = test_df.drop(columns=[self.TARGET_NAME])\n",
    "        y_test = test_df[self.TARGET_NAME]\n",
    "\n",
    "        test_df_1 = test_df[test_df['vas_id']==1]\n",
    "        X_test1 = test_df_1.drop(columns=['target'])\n",
    "        y_test1 = test_df_1['target']\n",
    "        y_pred1 = self.classifier1.predict(X_test1[self.columns1])\n",
    "        X_test1['y_pred'] = y_pred1\n",
    "        \n",
    "        test_df_2 = test_df[test_df['vas_id']==2]\n",
    "        X_test2 = test_df_2.drop(columns=['target'])\n",
    "        y_test2 = test_df_2['target']\n",
    "        y_pred2 = self.classifier2.predict(X_test2[self.columns2])\n",
    "        X_test2['y_pred'] = y_pred2\n",
    "        \n",
    "        test_df_4 = test_df[test_df['vas_id']==4]\n",
    "        X_test4 = test_df_4.drop(columns=['target'])\n",
    "        y_test4 = test_df_4['target']\n",
    "        y_pred4 = self.classifier4.predict(X_test4[self.columns4])\n",
    "        X_test4['y_pred'] = y_pred4\n",
    "        \n",
    "        test_df_5 = test_df[test_df['vas_id']==5]\n",
    "        X_test5 = test_df_5.drop(columns=['target'])\n",
    "        y_test5 = test_df_5['target']\n",
    "        y_pred5 = self.classifier5.predict(X_test5[self.columns5])\n",
    "        X_test5['y_pred'] = y_pred5\n",
    "        \n",
    "        test_df_6 = test_df[test_df['vas_id']==6]\n",
    "        X_test6 = test_df_6.drop(columns=['target'])\n",
    "        y_test6 = test_df_6['target']\n",
    "        y_pred6 = self.classifier6.predict(X_test6[self.columns6])\n",
    "        X_test6['y_pred'] = y_pred6\n",
    "        \n",
    "        test_df_7 = test_df[test_df['vas_id']==7]\n",
    "        X_test7 = test_df_7.drop(columns=['target'])\n",
    "        y_test7 = test_df_7['target']\n",
    "        y_pred7 = self.classifier7.predict(X_test7[self.columns7])\n",
    "        X_test7['y_pred'] = y_pred7\n",
    "        \n",
    "        test_df_8 = test_df[test_df['vas_id']==8]\n",
    "        X_test8 = test_df_8.drop(columns=['target'])\n",
    "        y_test8 = test_df_8['target']\n",
    "        y_pred8 = self.classifier8.predict(X_test8[self.columns8])\n",
    "        X_test8['y_pred'] = y_pred8\n",
    "        \n",
    "        test_df_9 = test_df[test_df['vas_id']==9]\n",
    "        X_test9 = test_df_9.drop(columns=['target'])\n",
    "        y_test9 = test_df_9['target']\n",
    "        y_pred9 = self.classifier9.predict(X_test9[self.columns9])\n",
    "        X_test9['y_pred'] = y_pred9\n",
    "        \n",
    "        y_pred = pd.DataFrame(pd.concat([X_test1['y_pred'], X_test2['y_pred'], X_test4['y_pred'],\n",
    "                        X_test5['y_pred'], X_test6['y_pred'], X_test7['y_pred'], \n",
    "                        X_test8['y_pred'], X_test9['y_pred']]))\n",
    "        predicted = pd.merge(X_test, y_pred, left_index=True, right_index=True, how='left')[['id', 'vas_id', 'buy_time_x', 'y_pred']]\n",
    "        print(classification_report(y_test, predicted['y_pred']))\n",
    "        predicted = predicted.rename(columns = {'y_pred': 'predict', 'buy_time_x': 'buy_time'})\n",
    "                \n",
    "        return predicted      \n",
    "         \n",
    "    def balance_df_by_target(df, target_name):\n",
    "\n",
    "        target_counts = df[target_name].value_counts()\n",
    "        major_class_name = target_counts.argmax()\n",
    "        minor_class_name = target_counts.argmin()\n",
    "        disbalance_coeff = int(target_counts[major_class_name] / target_counts[minor_class_name]) - 1\n",
    "        for i in range(disbalance_coeff):\n",
    "            sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name])\n",
    "            df = df.append(sample, ignore_index=True)\n",
    "        return df.sample(frac=1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "520755a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c5e3364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Estimator at 0x7f1f98e93dc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3956d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.97    308588\n",
      "         1.0       0.56      0.50      0.53     24074\n",
      "\n",
      "    accuracy                           0.94    332662\n",
      "   macro avg       0.76      0.73      0.75    332662\n",
      "weighted avg       0.93      0.94      0.93    332662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = estimator.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4bf94",
   "metadata": {},
   "source": [
    "Таким образом за счет обучения и настройки отдельных моделей по  vaz_id удалось поднять метрику на валидации на 7% при этом практически убрав переобучение. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e7fdc",
   "metadata": {},
   "source": [
    "Для кроссвалидации моделей пришлось бы сооружать общий пайплайн, а как показала практика составления пайплайнов, почти все пайплайны по разным vaz_id безбожно врут. Но кроссвалидация по отдельным моделям как правило почти не уменьшала метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbf2dc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vas_id</th>\n",
       "      <th>buy_time</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>305151</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1535317200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1547418</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1538341200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1229442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1531688400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1910129</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1544994000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3299282</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1532293200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  vas_id    buy_time  predict\n",
       "0   305151     2.0  1535317200      0.0\n",
       "1  1547418     1.0  1538341200      0.0\n",
       "2  1229442     1.0  1531688400      0.0\n",
       "3  1910129     6.0  1544994000      1.0\n",
       "4  3299282     2.0  1532293200      0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48357e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
